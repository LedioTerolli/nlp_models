{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.cElementTree as et\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import PlaintextCorpusReader, stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from gensim import corpora, utils\n",
    "from gensim.corpora import Dictionary\n",
    "from smart_open import open\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the path of xml file, in this case I am using the Posts.xml\n",
    "source_path = 'C:\\\\Users\\\\Terolli\\\\Documents\\\\Research Paper\\\\stackexchange\\\\stackoverflow.com-Posts\\\\Posts.xml'\n",
    "destination_path = 'C:\\\\Users\\\\Terolli\\\\Documents\\\\Research Paper\\\\stackexchange\\\\stackoverflow.com-Posts\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extracts documents not older than start_date (yyyy-mm)\n",
    "def extract_doc(start_date):\n",
    "    \n",
    "    prev = ['2008']\n",
    "    for event, elem in et.iterparse(source_path):\n",
    "\n",
    "        # used to check the progress of xml iteration\n",
    "        try:\n",
    "            current = elem.attrib.get('CreationDate')[:4]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        if current not in prev:\n",
    "            print(current)\n",
    "            prev.append(current)\n",
    "\n",
    "        # extracting document if not older than start_date\n",
    "        if start_date < elem.attrib.get('CreationDate'):\n",
    "\n",
    "            # getting the body of xml, which contains only the post with no metadata\n",
    "            soup = BeautifulSoup(elem.attrib.get('Body'), 'html.parser')\n",
    "\n",
    "            # separating natural language text\n",
    "            nlt_string = ''\n",
    "            for para in soup.find_all('p'):\n",
    "                nlt_string += para.get_text() + '\\n'\n",
    "\n",
    "            # separating code\n",
    "            code_string = ''\n",
    "            for code in soup.find_all('code'):\n",
    "                code_string += code.get_text() + '\\n'\n",
    "\n",
    "            # writing each post as 2 separate .txt files, one for natural language and the other for code\n",
    "            with open(destination_path + 'nlt\\\\nlt_' + elem.attrib.get('Id') + '.txt', \n",
    "                      'w+', encoding=\"utf-8\") as nlt_file:\n",
    "                if nlt_string != '':\n",
    "                    try:\n",
    "                        nlt_file.write(nlt_string)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        \n",
    "            with open(destination_path + 'code\\\\code_'+ elem.attrib.get('Id') + '.txt', \n",
    "                      'w+', encoding=\"utf-8\") as code_file:\n",
    "                if code_string != '':\n",
    "                    try:\n",
    "                        code_file.write(code_string)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "\n",
    "            nlt_file.close()\n",
    "            code_file.close()\n",
    "\n",
    "        # removing xml element from RAM\n",
    "        elem.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocesses the extracted natural language texts\n",
    "def preprocess(directory):\n",
    "    counter = 0;\n",
    "    for dirpath, dirs, filenames in os.walk(directory):\n",
    "        for file in filter(lambda file: file.endswith('.txt'), filenames):\n",
    "            try:\n",
    "                counter += 1\n",
    "                # convert txt file into continuous str\n",
    "                document = open(os.path.join(dirpath, file), encoding=\"utf8\").read()\n",
    "\n",
    "                # convert to lower case\n",
    "                document = document.lower()\n",
    "\n",
    "                # tokenize document\n",
    "                tk = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "                tokens = [token for token in tk.tokenize(document)]\n",
    "                \n",
    "                # lemmatize doc\n",
    "                lemma = WordNetLemmatizer()\n",
    "                tokens = [lemma.lemmatize(token) for token in tokens]\n",
    "                \n",
    "                # determine stop words\n",
    "                stoplist = set(stopwords.words('english'))\n",
    "                \n",
    "                # remove stop words\n",
    "                tokens = [token for token in tokens if token not in stoplist]\n",
    "\n",
    "                # remove words with length 1\n",
    "                tokens = [token for token in tokens if len(token) > 1]\n",
    "\n",
    "                # remove words with frequency == 1\n",
    "                frequency = defaultdict(int)\n",
    "                for token in tokens:\n",
    "                    frequency[token] += 1\n",
    "                tokens = [token for token in tokens if frequency[token] > 1]\n",
    "\n",
    "                # track progress\n",
    "                if counter%1000 == 0: print(counter)\n",
    "                    \n",
    "                yield tokens\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the corpus from the preprocessed documents\n",
    "class The_Corpus(object):\n",
    "\n",
    "    def __init__(self, dir):\n",
    "        self.dir = dir\n",
    "        self.dictionary = Dictionary(preprocess(dir))\n",
    "        self.dictionary.filter_extremes(no_below=20, no_above=0.4)\n",
    "        self.dictionary.compactify()\n",
    "        self.dictionary.save(destination_path + 'dictionary.dict')\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tokens in preprocess(self.dir):\n",
    "            yield self.dictionary.doc2bow(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the last 3 full months\n",
    "# last dataset update: 08-Sep-2020\n",
    "extract_doc('2020-06')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# saving the corpus\n",
    "corpora.MmCorpus.serialize(destination_path + 'the_corpus.mm', The_Corpus(destination_path + 'nlt'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
